
# Data Prep

Prior to building our models I need to prep the data sets by getting them in the format that I want. This is done through stratified data splitting and through K-fold Cross Validation. 

## Data Splitting 

First of all let us split our data. I have decided to split my data according to a 80/20 split, because I think this will leave the testing set with a decent number of observations. I am aware that my dataset is not the biggest data set, however, I still think that an 80/20 split is a good way to split the data so that the model can learn as much as possible. I have also set a seed of 14235 for the simple reason of consistency and reproducibility of my results, every time I run it. Finally, I have stratified the data on the response variable (price) so that the training data set is as close to the original data set as possible. 

```{r}
set.seed(14235)

house_split <- initial_split(house_trans, prop = 0.80, strata = price)
house_train <- training(house_split)

house_test <- testing(house_split)

```

Let us check the dimensions of the two data sets:

```{r}
dim(house_train)
```

```{r}
dim(house_test)
```

Here, we see that we have 1417 observations in my training data set and 356 in my testing data set. 


## K-Fold Cross Validation

I am also going to use cross validation to help with the issue of imbalanced data.

```{r}
house_folds <- vfold_cv(house_train, v = 10, repeats = 5)
house_folds
```

Because building models take so much computng time, I decided to save the results to an RDA file once I had the model I wanted so I could go back and load it later with no time commitment.

```{r}
load("/Users/janahindiyeh/Desktop/PSTAT 131 Final/house-Modeling-Setup.rda")
```


# Model Building
Now it is time for the actual model building. Before I delve into the specifics of how I am going to build each model, I wanted to give an overview of the process that I am going to be taken while building the models. While this is the most important section, it is the most tediious, which is why I saved all of the models that I ran in this section in a file and just called on them when running. The reason behind this is that each model took up to hours to run, which I could not afford every time I needed to run or fit the model. In the sections below, I have kept my code in and commented what each code does, but have not run each section every time. 

1. Building and running each model
  a. Setting up the recipe 
  b. Setting up the model specification (setting the mode as regression)
  c. Coding the workflow (Adding the model soec and the recipe)
  d. Setting up the tuning grid with the respective parameters for each model 
  e. Run the model 
2. Plotting the autoplots of each model
3. Analyzing the models (to pick the best model)
4. Selecting the best model
5. Testing the best model on the testing data set
  a. Fit the model to the testing data set 

## Recipe
Since each model is going to use the same model, I have created one recipe for all the models to use. This is essentially like a mini manual that each model will use and apply differently according to the purpose of each given model. I have used all the variables in the data (except id) because I believe that they all have a purpose in the model building section. After specifying the recipe itself, I have added *step_novel* which will assign a previously unseen factor level to a new value, then I used *step_dummy* to convert any character and factor variables into one or more numeric binary values. NExt, i used *step_zv* that removes all variables that have only a single value (or zero variance), and finally I used *step_normalize* to standardize (center and scale) the data so that it follows a stanard normal distribution curve (mean = 0 and standard deviation = 1).  

```{r}
house_recipe <- 
  recipe(formula = price ~., data = house_train) %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())


summary(house_recipe)
```

You can see above the summary of our recipe. This includes the variable name, the type of variable it is, the role that it plays (are all predictors escept for price which is the outcome variable), and the source (they are all original). 

## Ridge Regression 

Firstly, we will set up model specification, using hyperparametric tuning to find the ridge model that performs the best. 
```{r, eval = FALSE, echo = TRUE}
ridge_spec <- 
  linear_reg(penalty = tune(), mixture = 0) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

```

Next, I will create the associated workflow object. 
```{r, eval = FALSE, echo = TRUE}
ridge_wf <- workflow() %>% 
  add_recipe(house_recipe) %>% 
  add_model(ridge_spec)

```

I am also going to create a grid of the values of penalty I am going to be using. I have decided to range the penalty values from -5 to 5 with 50 levels 
```{r, eval = FALSE, echo = TRUE}
ridge_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)

```

Finally, I can now fit the ridge regression model using tune_grid
```{r, eval = FALSE, echo = TRUE}
tune_ridge <- tune_grid(
  ridge_wf,
  resamples = house_folds, 
  grid = ridge_grid
)

show_notes(.Last.tune.result)
```

```{r}
autoplot(tune_ridge)

```




## Random Forest 

```{r, eval = FALSE, echo = TRUE}
forest_spec <- rand_forest(mtry = tune()) %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("regression")
```

```{r, eval = FALSE, echo = TRUE}
forest_fit <- fit(forest_spec, price ~ ., data = (house_train))

```

```{r}
augment(forest_fit, new_data = house_train) %>%
  ggplot(aes(price, .pred)) +
  geom_abline() +
  geom_point(alpha = 0.5)
```

```{r}
vip(forest_fit)

```

```{r}
save(tune_ridge, tune_lasso, tune_tree, forest_fit, file = "~/Desktop/PSTAT 131 Final/models.rda")
```

```{r}
bagging_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("regression")

#Set up the random forest workflow, using the random forest model and formula 
bagging_wf <- workflow() %>% 
  add_model(bagging_spec) %>% 
  add_formula(price ~ .)

rand_grid <- grid_regular(mtry(range = c(1, 25)), trees(range = c(200 ,100)), min_n(range = c(1,3)), levels = 5)

forest_tune_res <- tune_grid(
  bagging_wf, 
  resamples = house_folds, 
  grid = rand_grid
)
```

```{r}
library(xgboost)
#Set up a boosted tree model, using the xgboost engine, tuning trees 
boosted_spec <- boost_tree(trees = tune(), tree_depth = 4) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

#Create a regular grid with 10 levels, let trees range from 10 to 2000
boosted_grid <- grid_regular(trees(range = c(10,2000)), levels = 10)

#Set up the boosted tree workflow 
boosted_wf <- workflow() %>% 
  add_model(boosted_spec) %>% 
  add_formula(price ~.)


#Tune the model, specifying roc_auc as the metric 
tune_boosted <- tune_grid(
  boosted_wf, 
  resamples = house_folds, 
  grid = boosted_grid, 
)

#Print an autoplot of the results 
autoplot(tune_boosted)

```









